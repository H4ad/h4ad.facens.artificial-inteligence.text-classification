{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação Continuada 2 (AC2) - Dados de Treinamento\n",
    "\n",
    "Abaixo, iremos criar um modelo para classificar mensagens de spam a partir de um dataset obtido no Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Primeiro, começamos a importar todas as bibliotecas que iremos precisar nesse notebook, apenas para simplificar mais tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos módulos\n",
    "from __future__ import print_function\n",
    "\n",
    "# biblioteca usada para trabalhar com dataframes\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# uma biblioteca usada para auxiliar a limpeza de tags html\n",
    "import html\n",
    "\n",
    "# bibliotecas básicas como numpy e matplotlib para exibir \n",
    "# alguns gráficos caso seja necessário. \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# biblioteca para usar para medir acuracia e separar os dados de treino e teste\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# biblioteca usada para criar modelos de word embedding\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "import re # biblioteca para expressoes regulares\n",
    "\n",
    "import nltk # biblioteca para Processamento de Linguagem Natural\n",
    "from nltk.stem.porter import PorterStemmer # para fazer a estemização em documentos da lingua inglesa\n",
    "from nltk.stem import RSLPStemmer # para fazer a estemização em documentos da lingua portuguesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, um import que deve ser executado apenas um vez, caso não tenha executado, descomente a linha e execute novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all') # instala todos os recursos da biblioteca NLTK. Você pode descomentar esta linha na 1a vez que for executar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Depois, inicializamos o nosso dataset que foi baixado do Kaggle e pode ser obtido a partir [desse link](https://www.kaggle.com/team-ai/spam-text-message-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_url = \"https://raw.githubusercontent.com/H4ad/h4ad.facens.artificial-inteligence.text-classification/master/datasets/spam-text-message-20170820.csv\"\n",
    "data_set = pd.read_csv(data_set_url,sep=\",\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória\n",
    "\n",
    "Abaixo, um pouco dos dados que iremos ver e tratar durante o notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que há dentro?\n",
    "\n",
    "Vamos começar primeiro a visualizar o que há dentro desse dataset com alguns comandos que nos mostram alguns informações básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                 Message\n",
       "count      5572                    5572\n",
       "unique        2                    5157\n",
       "top         ham  Sorry, I'll call later\n",
       "freq       4825                      30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_set.head(n=5))\n",
    "display(data_set.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores Duplicados\n",
    "\n",
    "Na tabela acima, podemos ver que na coluna \"Message\", temos um total de 5572 valores mas apenas 5157 são únicos.\n",
    "\n",
    "Para não influenciar ou causar algum problema durante os treinos, rodar um outro comando para realmente ter a contagem de quantos dados duplicados nós temos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O número é de 415, é bem expressivo. Dessa forma, vamos agora tratar de apagar esses valores duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape antes de remover os dados.\n",
      "(5572, 2)\n",
      "\n",
      "Shape após remover os dados.\n",
      "(5157, 2)\n",
      "\n",
      "Quantidade de itens removidos: 415\n"
     ]
    }
   ],
   "source": [
    "print('Shape antes de remover os dados.')\n",
    "print(data_set.shape)\n",
    "\n",
    "data_set_cleaned = data_set.drop_duplicates(inplace=False)\n",
    "\n",
    "print('\\nShape após remover os dados.')\n",
    "print(data_set_cleaned.shape)\n",
    "\n",
    "print('\\nQuantidade de itens removidos:', (data_set.shape[0] - data_set_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores Faltando\n",
    "\n",
    "Após checar valores duplicados, podemos também verificar se não há algum valor faltando, usando o comando abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0.0\n",
       "Message     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_cleaned.isna().sum() / data_set_cleaned.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E como podemos ver, tudo zero para ambas as colunas, dessa forma, não precisamos fazer mais nada já que não há valores nulos ou faltando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento\n",
    "\n",
    "Após essa analise inicial dos dados, vamos começar com a etapa de pré-processamento dos textos obtidos.\n",
    "\n",
    "As técnicas e o código abaixo usado foi obtido e modificado a partir do código original escrito pelo professor Renato Moraes Silva que pode ser encontrado [nesse notebook.](https://facens.instructure.com/courses/7531/files/1356091?module_item_id=269232)\n",
    "\n",
    "\n",
    "Sobre as técnicas, serão usadas as seguintes:\n",
    "\n",
    " - deixar todas as palavras com letras minúsculas\n",
    " - substituir os números pela tag *\\<NUMBER\\>*\n",
    " - substituir todas as URLS pela tag *\\<URL\\>*\n",
    " - substituir todos os emails pela tag *\\<EMAIL\\>*\n",
    " - substituir o símbolo de moeda pela tag *\\<MONEY\\>*\n",
    " - substituir todos os caracteres não-alfanuméricos por um espaço em branco\n",
    "\n",
    "E o que foi adicionado além das técnicas acima foram:\n",
    " - remover as entidades do html (`&gt;` `&#62;`).\n",
    "\n",
    "Além disso, será aplicado também um processo de *estemização*, no qual diminui uma palavra para o seu radical, tornando uma palavra como \"flies\" para \"fli\". \n",
    "\n",
    "E também, será aplicado um processo de remoção das *stopwords*, no qual remove palavras muito comuns de uma lingua. Como estamos usando um dataset em inglês, palavras como \"i\", \"me\", \"my\", \"myself\", \"we\" e muitas outras.\n",
    "\n",
    "> Uma lista de exemplos mais detalhadas pode ser encontrado [nesse gist](https://gist.github.com/sebleier/554280)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stemming = False, stopwords = False):\n",
    "    \"\"\"\n",
    "    Funcao usada para fazer o tratamento de textos da lingua inglesa\n",
    "    \n",
    "    Parametros: \n",
    "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
    "        \n",
    "        stemming: variavel do tipo booleana que indica se a estemizacao deve ser aplicada ou nao\n",
    "        \n",
    "        stopwords: variavel do tipo booleana que indica se as stopwords devem ser removidas ou nao\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove tags HTML\n",
    "    regex = re.compile('<[^<>]+>')\n",
    "    text = re.sub(regex, \" \", text) \n",
    "    \n",
    "    # remove as entidades do HTML\n",
    "    regex = re.compile('(&.+;)')\n",
    "    text = re.sub(regex, \" \", text) \n",
    "    \n",
    "    # normaliza as URLs\n",
    "    regex = re.compile('(http|https)://[^\\s]*')\n",
    "    text = re.sub(regex, \"<URL>\", text)\n",
    "\n",
    "    # normaliza emails\n",
    "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
    "    text = re.sub(regex, \"<EMAIL>\", text)\n",
    "    \n",
    "    #normaliza o símbolo de dólar\n",
    "    regex = re.compile('[$]+')\n",
    "    text = re.sub(regex, \"<MONEY>\", text)\n",
    "    \n",
    "    # converte todos os caracteres não-alfanuméricos em espaço\n",
    "    regex = re.compile('[^A-Za-z0-9]+')  \n",
    "    text = re.sub(regex, \" \", text)\n",
    "    \n",
    "    # normaliza os numeros \n",
    "    regex = re.compile('[0-9]+')\n",
    "    text = re.sub(regex, \"<NUMBER>\", text)\n",
    "    \n",
    "    # substitui varios espaçamentos seguidos em um só\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopwords:\n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [w for w in words if not w in nltk.corpus.stopwords.words('english')]\n",
    "        text = \" \".join( words )\n",
    "    \n",
    "    # aplica estemização\n",
    "    if stemming: \n",
    "        stemmer_method = PorterStemmer()  \n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [ stemmer_method.stem(w) for w in words ]\n",
    "        text = \" \".join( words )\n",
    "    \n",
    "    # remove palavras de apenas um caracter\n",
    "    words = text.split() # separa o texto em palavras\n",
    "    words = [ w for w in words if len(w)>1 ]\n",
    "    text = \" \".join( words )\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função acima, podemos agora chamar ela para realizar todo o pré-processamento do nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um dos textos antes do processamento:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I sent you  &lt;#&gt;  bucks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O mesmo texto, só que agora processado:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sent you bucks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Um dos textos antes do processamento:\")\n",
    "display(data_set_cleaned[\"Message\"][201])\n",
    "\n",
    "data_set_processed = data_set_cleaned.copy()\n",
    "data_set_processed[\"Message\"] = data_set_processed[\"Message\"].apply(preprocessing)\n",
    "\n",
    "print(\"O mesmo texto, só que agora processado:\")\n",
    "display(data_set_processed[\"Message\"][201])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding (Transformando texto em número)\n",
    "\n",
    "Após realizar o pré-processamento, temos todos os nossos textos e palavras bem organizadas e formatadas. Dessa forma, o que devemos fazer é transformar essas palavras em um formato que o nosso modelo de decisão em arvore irá conseguir entender, e é ai que entra o Word Embedding.\n",
    "\n",
    "Word Embedding é uma técnica para transformar um texto em um vetor de atributos com valores numericos. Há alguns pré-treinados, como o da Google, mas iremos criar um próprio porque os pré-treinados são gigantescos (na ordem de Gigas) e serão muito potentes para um simples experimento.\n",
    "\n",
    "Assim, iremos começar agora a criar uma lista de lista com as palavras de cada mensagem no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras que iremos usar para montar o nosso vocabulário:\n",
      "['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'great', 'world', 'la', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n"
     ]
    }
   ],
   "source": [
    "sentences = data_set_processed[\"Message\"].apply(lambda message: message.split()).tolist()\n",
    "\n",
    "print(\"Algumas palavras que iremos usar para montar o nosso vocabulário:\")\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E com essa lista de palavras, iremos agora treinar o modelo de Word Embedding utilizando a biblioteca [Genshim](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, vector_size=200, window=3, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o modelo treinado, podemos ver qual é o tamanho do vocabulário da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulario:  3632\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do Vocabulario: ', + len(model.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se precisarmos obter o valor associado a cada palavra, podemos usar o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17709242 -0.12600343 -0.08791727  0.16236502  0.28659806 -0.35685495\n",
      "  0.19971345  0.65286946 -0.19657624  0.24297376 -0.24326402 -0.39725733\n",
      "  0.12053519  0.20995583 -0.08815522 -0.35241863 -0.08457894 -0.01442036\n",
      "  0.02464742 -0.509421    0.22960162 -0.31073946 -0.1612563  -0.12143413\n",
      "  0.09649967 -0.23418328 -0.04336007 -0.14399362 -0.34557962  0.15271045\n",
      "  0.4199807   0.01149173  0.16687284 -0.08555082 -0.08175757  0.23091978\n",
      "  0.29245043 -0.14143606 -0.07441217 -0.520712   -0.3377079  -0.09030245\n",
      "  0.00683281  0.11942589  0.5297194  -0.14374237 -0.22596823 -0.09825926\n",
      "  0.1655615   0.21038999  0.00480481 -0.15803435 -0.22627462 -0.30204716\n",
      "  0.08603162 -0.0929756  -0.01578389 -0.31015152 -0.31742713  0.14342752\n",
      " -0.00655344  0.04587211 -0.00159489 -0.06823715 -0.62862366  0.15215392\n",
      "  0.01981192  0.6696914  -0.31566703  0.5178813  -0.02604795  0.01275099\n",
      "  0.36037377  0.03543808  0.11258061  0.20872577  0.29203978 -0.2529166\n",
      " -0.5108547  -0.05823629 -0.22444083 -0.07205907 -0.4914758   0.66352636\n",
      " -0.28399932 -0.04438004  0.02532425  0.32328814  0.03616106 -0.21636955\n",
      "  0.22496296  0.3708162   0.24226017  0.42886463  0.63994503  0.3560146\n",
      "  0.18308473 -0.383155    0.3039771   0.24608232 -0.5296679   0.525161\n",
      "  0.19315515 -0.19300114 -0.34198707 -0.38690123  0.13990176  0.4910308\n",
      " -0.2664813  -0.44514963 -0.05517951 -0.4822203  -0.10223246 -0.08747105\n",
      "  0.16128018 -0.09715439  0.29186636 -0.66374016 -0.27256036 -0.31009215\n",
      "  0.01626453  0.358692    0.22000948 -0.3095225  -0.28936586  0.33119848\n",
      " -0.2234851  -0.17560482 -0.19764991  0.03951213  0.26213145  0.14590995\n",
      " -0.23713557 -0.24072139 -0.1791775   0.46712127 -0.23019785 -0.4189512\n",
      " -0.14280371 -0.4440838   0.29100922 -0.48612213 -0.09494057 -0.10188647\n",
      "  0.03657534 -0.15512145 -0.20774244  0.13089678  0.01562879  0.04265926\n",
      "  0.14750503 -0.38053557  0.07111282  0.16032405 -0.7364456   0.53384036\n",
      "  0.34395456  0.4036722  -0.05632755  0.16705011  0.25496936  0.22363456\n",
      " -0.10725677 -0.08006395 -0.0036669   0.19019288  0.15015027 -0.28019375\n",
      " -0.24264525  0.10334086 -0.31922644  0.21064484 -0.03770718 -0.23056032\n",
      "  0.19107653  0.21827991 -0.29539868  0.15712824  0.15595573  0.19411577\n",
      " -0.0206822   0.02931563  0.02809272  0.06188481  0.11992608  0.04438627\n",
      " -0.06380086  0.17258039  0.4319862   0.07950979  0.4226515   0.05209548\n",
      " -0.24226308 -0.2358219   0.22758181  0.28771067  0.2704624  -0.4518621\n",
      " -0.01927218 -0.12566972]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"call\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E é possível até mesmo verificar a similaridade dessa palavra com outras, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('claim', 0.9994510412216187), ('prize', 0.9992076754570007), ('free', 0.9992011785507202), ('cash', 0.9991630911827087), ('or', 0.9989598989486694), ('guaranteed', 0.998924195766449), ('<NUMBER>p', 0.9988560676574707), ('reply', 0.9988375902175903), ('txt', 0.9988107085227966), ('stop', 0.998788595199585)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"call\", topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, agora que foi visto tudo que podemos fazer usando o modelo de Word Embedding, podemos usar ele para transformar todo o nosso dataset para um formato que será possível usar ele para o treinamento.\n",
    "\n",
    "Vamos começar definindo uma função que irá transformar uma sentença inteira um vetor de números.\n",
    "\n",
    "> A função abaixo é uma ligeira adaptação do código do professor Renato, e pode ser encontrado [nesse link.](https://facens.instructure.com/courses/7531/files/1356091?module_item_id=269232) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando com uma frase qualquer\n",
      "[ 0.09511602 -0.07759578 -0.06158502  0.07376378  0.19277689 -0.20688233\n",
      "  0.07275587  0.35907674 -0.122722    0.11012122 -0.15914634 -0.20448256\n",
      "  0.07375097  0.1316086  -0.04187575 -0.1842224  -0.06262719 -0.00350946\n",
      "  0.00449819 -0.27489054  0.1157137  -0.14582513 -0.06547634 -0.08518134\n",
      "  0.05659532 -0.12771186 -0.03049851 -0.08392287 -0.18977188  0.10628672\n",
      "  0.22717555  0.00103121  0.07097273 -0.0234408  -0.03558218  0.12889321\n",
      "  0.13224864 -0.07967652 -0.03841361 -0.31163293 -0.1613926  -0.02067659\n",
      "  0.00953143  0.06929825  0.3083307  -0.05120663 -0.122779   -0.08409563\n",
      "  0.09526726  0.10617401  0.01536868 -0.10533574 -0.12229341 -0.15787761\n",
      "  0.03991915 -0.03350319 -0.02107173 -0.16109027 -0.17343828  0.06413537\n",
      "  0.00376832  0.01990785 -0.0126125  -0.03591307 -0.33706772  0.0938308\n",
      "  0.01945116  0.3537798  -0.1846274   0.29767275 -0.02473962  0.01160061\n",
      "  0.2064071   0.01261324  0.06667463  0.10853186  0.16218923 -0.16665506\n",
      " -0.315526   -0.00947772 -0.13563074 -0.03416011 -0.2657318   0.3810616\n",
      " -0.14286676 -0.03498578  0.00102127  0.17988737  0.01817887 -0.1253468\n",
      "  0.12845281  0.20720726  0.13542384  0.2488243   0.34597382  0.19217588\n",
      "  0.11121635 -0.18653631  0.16611493  0.14163221 -0.27675998  0.29547155\n",
      "  0.09546699 -0.09715563 -0.19605984 -0.21078597  0.09045704  0.28740394\n",
      " -0.14670695 -0.24118581 -0.03864143 -0.25963232 -0.05979757 -0.06007187\n",
      "  0.07473593 -0.07712239  0.15020001 -0.3822816  -0.14708646 -0.16921543\n",
      " -0.00328691  0.19443168  0.10477232 -0.14534211 -0.16419171  0.16431208\n",
      " -0.10160886 -0.12487718 -0.09594154 -0.01945306  0.1411422   0.0629193\n",
      " -0.11518905 -0.1216517  -0.07955277  0.23850508 -0.12455299 -0.21911696\n",
      " -0.10315885 -0.25191644  0.15321311 -0.25676498 -0.05529976 -0.07242206\n",
      " -0.00255147 -0.0690228  -0.12752132  0.07603656  0.01267408  0.02150336\n",
      "  0.07124678 -0.20373027  0.03305297  0.08695951 -0.3865552   0.29042703\n",
      "  0.17643446  0.2079094  -0.03796596  0.09967975  0.15078187  0.09996457\n",
      " -0.05407874 -0.03834902 -0.00280577  0.10676661  0.0820597  -0.15206085\n",
      " -0.16127518  0.02813702 -0.16760743  0.11065552 -0.0147209  -0.13375062\n",
      "  0.09447146  0.13215874 -0.16261801  0.09605756  0.06978287  0.11830647\n",
      " -0.01248021  0.02043894 -0.01318358  0.04187402  0.08930422  0.02960132\n",
      " -0.02423713  0.13134801  0.23594949  0.03531897  0.22772329  0.03226257\n",
      " -0.13164549 -0.09589208  0.11243307  0.14946042  0.1486481  -0.25163817\n",
      "  0.00345272 -0.07269315]\n"
     ]
    }
   ],
   "source": [
    "def getDocvector(sentence):\n",
    "  sentenceValues = []\n",
    "\n",
    "  for word in sentence:\n",
    "    # É interessante notar o uso do try aqui dentro,\n",
    "    # ele se dá porque, durante a criação do modelo,\n",
    "    # nós especificamos que ele só gere o modelo com\n",
    "    # palavras que aparecem ao menos 2 vezes. Dessa \n",
    "    # forma, palavras com frequencia menor que isso\n",
    "    # são simplesmente ignoradas, e ai podem causar \n",
    "    # que ao tentar acessar elas, ocorra uma exceção\n",
    "    # porque ela não existe no modelo.\n",
    "    try:\n",
    "      sentenceValues.append(model.wv[word])\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  if len(sentenceValues) > 0:\n",
    "    return np.mean(sentenceValues, axis = 0)\n",
    "\n",
    "  return np.zeros(model.wv.vector_size)\n",
    "\n",
    "print('Testando com uma frase qualquer')\n",
    "print(getDocvector(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função acima, podemos agora transformar todas as nossas mensagens do modelo para um vetores de valores, assim como, separar nossos valores de classificação e valores de treino e teste, da seguinte forma:\n",
    "\n",
    "- x = Valores que podem ser usados para descrever se é spam ou não\n",
    "- y = A informação se é realmente um spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do vetor de classe:  (5157,)\n",
      "Shape do vetor de valores:  (5157, 200)\n"
     ]
    }
   ],
   "source": [
    "y = data_set_processed[\"Category\"].to_numpy()\n",
    "x = []\n",
    "\n",
    "for sentence in data_set_processed[\"Message\"]:\n",
    "  x.append(getDocvector(sentence.split()))\n",
    "  \n",
    "x = np.array(x)\n",
    "\n",
    "print(\"Shape do vetor de classe: \", y.shape)\n",
    "print(\"Shape do vetor de valores: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando os Dados\n",
    "\n",
    "Bom, após ter o modelo para poder transformar os textos em números, podemos agora começar a treinar o nosso modelo, mas antes, precisamos separar os dados que serão usados no treino.\n",
    "\n",
    "Vamos separar os valores em:\n",
    "\n",
    "- x_train = Valores de treino\n",
    "- y_train = A classe se diz se é spam ou não de treino\n",
    "- x_test = Valores para testar o modelo após o treino\n",
    "- y_test = A classe dizendo se é spam ou não de teste para testar o modelo após o treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, vamos nos certificar que a tipagem dos valores estejam corretos, ou seja, todos sejam do tipo float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float64)\n",
    "x_test = x_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, temos as seguintes métricas sobre os nossos dados de teste e treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qntd. de amostras de treino:  3609\n",
      "Qntd de amostras de teste:  1548\n",
      "\n",
      "Qtd. de dados de cada classe (treinamento)\n",
      "\tClasse ham:  3164\n",
      "\tClasse spam:  445\n",
      "\n",
      "Qtd. de dados de cada classe (teste)\n",
      "\tClasse ham:  1352\n",
      "\tClasse spam:  196\n"
     ]
    }
   ],
   "source": [
    "print(\"Qntd. de amostras de treino: \", x_train.shape[0])\n",
    "print(\"Qntd de amostras de teste: \", x_test.shape[0])\n",
    "\n",
    "print(\"\\nQtd. de dados de cada classe (treinamento)\")\n",
    "print(\"\\tClasse ham: \", len(y_train[y_train == 'ham']))\n",
    "print(\"\\tClasse spam: \", len(y_train[y_train == 'spam']))\n",
    "\n",
    "print(\"\\nQtd. de dados de cada classe (teste)\")\n",
    "print(\"\\tClasse ham: \", len(y_test[y_test == 'ham']))\n",
    "print(\"\\tClasse spam: \", len(y_test[y_test == 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tudo pronto\n",
    "\n",
    "Dessa forma, com os dados do nosso dataset preparado, podemos agora ir para os outros notebooks para rodar os métodos de classificação."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "177401c68bf2cebd86e5b16616fdddb5ae8a22ec2666d9ac189b12bd387b0870"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
